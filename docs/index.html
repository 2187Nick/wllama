<!DOCTYPE html><html class="default" lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="IE=edge"/><title>wllama</title><meta name="description" content="Documentation for wllama"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="assets/style.css"/><link rel="stylesheet" href="assets/highlight.css"/><script defer src="assets/main.js"></script><script async src="assets/icons.js" id="tsd-icons-script"></script><script async src="assets/search.js" id="tsd-search-script"></script><script async src="assets/navigation.js" id="tsd-nav-script"></script></head><body><script>document.documentElement.dataset.theme = localStorage.getItem("tsd-theme") || "os";document.body.style.display="none";setTimeout(() => app?app.showPage():document.body.style.removeProperty("display"),500)</script><header class="tsd-page-toolbar"><div class="tsd-toolbar-contents container"><div class="table-cell" id="tsd-search" data-base="."><div class="field"><label for="tsd-search-field" class="tsd-widget tsd-toolbar-icon search no-caption"><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><use href="assets/icons.svg#icon-search"></use></svg></label><input type="text" id="tsd-search-field" aria-label="Search"/></div><div class="field"><div id="tsd-toolbar-links"></div></div><ul class="results"><li class="state loading">Preparing search index...</li><li class="state failure">The search index is not available</li></ul><a href="index.html" class="title">wllama</a></div><div class="table-cell" id="tsd-widgets"><a href="#" class="tsd-widget tsd-toolbar-icon menu no-caption" data-toggle="menu" aria-label="Menu"><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><use href="assets/icons.svg#icon-menu"></use></svg></a></div></div></header><div class="container container-main"><div class="col-content"><div class="tsd-page-title"><h2>wllama</h2></div><div class="tsd-panel tsd-typography"><a id="md:wllama---wasm-binding-for-llamacpp" class="tsd-anchor"></a><h1><a href="#md:wllama---wasm-binding-for-llamacpp">wllama - Wasm binding for llama.cpp</a></h1><p><img src="./README_banner.png" alt=""></p>
<p>Another WebAssembly binding for <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>. Inspired by <a href="https://github.com/tangledgroup/llama-cpp-wasm">tangledgroup/llama-cpp-wasm</a>, but unlike it, <strong>wllama</strong> aims to supports <strong>low-level API</strong> like (de)tokenization, embeddings,...</p>
<p>Wasm allow llama.cpp to run directly on browser, without any server part.</p>
<a id="md:features" class="tsd-anchor"></a><h2><a href="#md:features">Features</a></h2><ul>
<li>Typescript support</li>
<li>High-level API: completions, embeddings</li>
<li>Low-level API: (de)tokenize, KV cache control, sampling control,...</li>
<li>Ability to load splitted model</li>
<li>Auto switch between single-thread and multi-thread build based on browser support</li>
</ul>
<a id="md:demo-and-documentations" class="tsd-anchor"></a><h2><a href="#md:demo-and-documentations">Demo and documentations</a></h2><p>Documentation: <a href="https://ngxson.github.io/docs/">https://ngxson.github.io/docs/</a></p>
<p>Demo:</p>
<ul>
<li>Basic usages with completions and embeddings: <a href="https://ngxson.github.io/wllama/examples/basic/">https://ngxson.github.io/wllama/examples/basic/</a></li>
</ul>
<a id="md:how-to-use" class="tsd-anchor"></a><h2><a href="#md:how-to-use">How to use</a></h2><p>See in <code>examples</code></p>
<pre><code class="language-javascript"><span class="hl-0">import</span><span class="hl-1"> { </span><span class="hl-2">Wllama</span><span class="hl-1"> } </span><span class="hl-0">from</span><span class="hl-1"> </span><span class="hl-3">&#39;../../esm/index.js&#39;</span><span class="hl-1">;</span><br/><br/><span class="hl-1">(</span><span class="hl-4">async</span><span class="hl-1"> () </span><span class="hl-4">=&gt;</span><span class="hl-1"> {</span><br/><span class="hl-1">  </span><span class="hl-5">// Automatically switch between single-thread and multi-thread version based on browser support</span><br/><span class="hl-1">  </span><span class="hl-5">// If you want to enforce single-thread, remove &quot;wasmMultiThreadPath&quot; and &quot;workerMultiThreadPath&quot;</span><br/><span class="hl-1">  </span><span class="hl-4">const</span><span class="hl-1"> </span><span class="hl-6">wllama</span><span class="hl-1"> = </span><span class="hl-4">new</span><span class="hl-1"> </span><span class="hl-7">Wllama</span><span class="hl-1">({</span><br/><span class="hl-1">    </span><span class="hl-2">wasmSingleThreadPath:</span><span class="hl-1"> </span><span class="hl-3">&#39;../../esm/single-thread/wllama.wasm&#39;</span><span class="hl-1">,</span><br/><span class="hl-1">    </span><span class="hl-2">wasmMultiThreadPath:</span><span class="hl-1"> </span><span class="hl-3">&#39;../../esm/multi-thread/wllama.wasm&#39;</span><span class="hl-1">,</span><br/><span class="hl-1">    </span><span class="hl-2">workerMultiThreadPath:</span><span class="hl-1"> </span><span class="hl-3">&#39;../../esm/multi-thread/wllama.worker.mjs&#39;</span><span class="hl-1">,</span><br/><span class="hl-1">  });</span><br/><span class="hl-1">  </span><span class="hl-0">await</span><span class="hl-1"> </span><span class="hl-2">wllama</span><span class="hl-1">.</span><span class="hl-7">loadModel</span><span class="hl-1">(</span><span class="hl-3">&#39;https://huggingface.co/ggml-org/models/resolve/main/tinyllamas/stories260K.gguf&#39;</span><span class="hl-1">, {});</span><br/><span class="hl-1">  </span><span class="hl-4">const</span><span class="hl-1"> </span><span class="hl-6">outputText</span><span class="hl-1"> = </span><span class="hl-0">await</span><span class="hl-1"> </span><span class="hl-2">wllama</span><span class="hl-1">.</span><span class="hl-7">createCompletion</span><span class="hl-1">(</span><span class="hl-2">elemInput</span><span class="hl-1">.</span><span class="hl-2">value</span><span class="hl-1">, {</span><br/><span class="hl-1">    </span><span class="hl-2">nPredict:</span><span class="hl-1"> </span><span class="hl-8">50</span><span class="hl-1">,</span><br/><span class="hl-1">    </span><span class="hl-2">sampling:</span><span class="hl-1"> {</span><br/><span class="hl-1">      </span><span class="hl-2">temp:</span><span class="hl-1"> </span><span class="hl-8">0.5</span><span class="hl-1">,</span><br/><span class="hl-1">      </span><span class="hl-2">top_k:</span><span class="hl-1"> </span><span class="hl-8">40</span><span class="hl-1">,</span><br/><span class="hl-1">      </span><span class="hl-2">top_p:</span><span class="hl-1"> </span><span class="hl-8">0.9</span><span class="hl-1">,</span><br/><span class="hl-1">    },</span><br/><span class="hl-1">  });</span><br/><span class="hl-1">  </span><span class="hl-2">console</span><span class="hl-1">.</span><span class="hl-7">log</span><span class="hl-1">(</span><span class="hl-2">outputText</span><span class="hl-1">);</span><br/><span class="hl-1">})();</span>
</code><button>Copy</button></pre>
<a id="md:how-to-build" class="tsd-anchor"></a><h2><a href="#md:how-to-build">How to build</a></h2><p>This repository already come with pre-built binary. But if you want to build it yourself, you can use the commands below:</p>
<pre><code class="language-shell"><span class="hl-5"># Require having docker compose installed</span><br/><span class="hl-5"># Firstly, build llama.cpp into wasm</span><br/><span class="hl-7">npm</span><span class="hl-1"> </span><span class="hl-3">run</span><span class="hl-1"> </span><span class="hl-3">build:wasm</span><br/><span class="hl-5"># (Optionally) Build ES6 module</span><br/><span class="hl-7">npm</span><span class="hl-1"> </span><span class="hl-3">run</span><span class="hl-1"> </span><span class="hl-3">build</span>
</code><button>Copy</button></pre>
<a id="md:todo" class="tsd-anchor"></a><h2><a href="#md:todo">TODO</a></h2><ul>
<li>Deploy to npm</li>
<li>Guide: How to split gguf file?</li>
<li>Support multi-sequences: knowing the resource limitation when using WASM, I don&#39;t think having multi-sequences is a good idea</li>
<li>Multi-modal: Waiting for refactoring LLaVA implementation from llama.cpp</li>
</ul>
</div></div><div class="col-sidebar"><div class="page-menu"><div class="tsd-navigation settings"><details class="tsd-index-accordion"><summary class="tsd-accordion-summary"><h3><svg width="20" height="20" viewBox="0 0 24 24" fill="none"><use href="assets/icons.svg#icon-chevronDown"></use></svg>Settings</h3></summary><div class="tsd-accordion-details"><div class="tsd-filter-visibility"><h4 class="uppercase">Member Visibility</h4><form><ul id="tsd-filter-options"><li class="tsd-filter-item"><label class="tsd-filter-input"><input type="checkbox" id="tsd-filter-protected" name="protected"/><svg width="32" height="32" viewBox="0 0 32 32" aria-hidden="true"><rect class="tsd-checkbox-background" width="30" height="30" x="1" y="1" rx="6" fill="none"></rect><path class="tsd-checkbox-checkmark" d="M8.35422 16.8214L13.2143 21.75L24.6458 10.25" stroke="none" stroke-width="3.5" stroke-linejoin="round" fill="none"></path></svg><span>Protected</span></label></li><li class="tsd-filter-item"><label class="tsd-filter-input"><input type="checkbox" id="tsd-filter-private" name="private"/><svg width="32" height="32" viewBox="0 0 32 32" aria-hidden="true"><rect class="tsd-checkbox-background" width="30" height="30" x="1" y="1" rx="6" fill="none"></rect><path class="tsd-checkbox-checkmark" d="M8.35422 16.8214L13.2143 21.75L24.6458 10.25" stroke="none" stroke-width="3.5" stroke-linejoin="round" fill="none"></path></svg><span>Private</span></label></li><li class="tsd-filter-item"><label class="tsd-filter-input"><input type="checkbox" id="tsd-filter-inherited" name="inherited" checked/><svg width="32" height="32" viewBox="0 0 32 32" aria-hidden="true"><rect class="tsd-checkbox-background" width="30" height="30" x="1" y="1" rx="6" fill="none"></rect><path class="tsd-checkbox-checkmark" d="M8.35422 16.8214L13.2143 21.75L24.6458 10.25" stroke="none" stroke-width="3.5" stroke-linejoin="round" fill="none"></path></svg><span>Inherited</span></label></li><li class="tsd-filter-item"><label class="tsd-filter-input"><input type="checkbox" id="tsd-filter-external" name="external"/><svg width="32" height="32" viewBox="0 0 32 32" aria-hidden="true"><rect class="tsd-checkbox-background" width="30" height="30" x="1" y="1" rx="6" fill="none"></rect><path class="tsd-checkbox-checkmark" d="M8.35422 16.8214L13.2143 21.75L24.6458 10.25" stroke="none" stroke-width="3.5" stroke-linejoin="round" fill="none"></path></svg><span>External</span></label></li></ul></form></div><div class="tsd-theme-toggle"><h4 class="uppercase">Theme</h4><select id="tsd-theme"><option value="os">OS</option><option value="light">Light</option><option value="dark">Dark</option></select></div></div></details></div><details open class="tsd-index-accordion tsd-page-navigation"><summary class="tsd-accordion-summary"><h3><svg width="20" height="20" viewBox="0 0 24 24" fill="none"><use href="assets/icons.svg#icon-chevronDown"></use></svg>On This Page</h3></summary><div class="tsd-accordion-details"><a href="#md:wllama---wasm-binding-for-llamacpp"><span>wllama -<wbr/> <wbr/>Wasm binding for llama.cpp</span></a><ul><li><a href="#md:features"><span>Features</span></a></li><li><a href="#md:demo-and-documentations"><span>Demo and documentations</span></a></li><li><a href="#md:how-to-use"><span>How to use</span></a></li><li><a href="#md:how-to-build"><span>How to build</span></a></li><li><a href="#md:todo"><span>TODO</span></a></li></ul></div></details></div><div class="site-menu"><nav class="tsd-navigation"><a href="modules.html" class="current"><svg class="tsd-kind-icon" viewBox="0 0 24 24"><use href="assets/icons.svg#icon-1"></use></svg><span>wllama</span></a><ul class="tsd-small-nested-navigation" id="tsd-nav-container" data-base="."><li>Loading...</li></ul></nav></div></div></div><div class="tsd-generator"><p>Generated using <a href="https://typedoc.org/" target="_blank">TypeDoc</a></p></div><div class="overlay"></div></body></html>